<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Huan Zheng, 郑欢">
<link rel="stylesheet" href="./jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/DMU_64.ico">
<title>Huan Zheng</title>
</head>


<body>

<a id="home" class="anchor"></a>
<div id="container">
<div class="container">

<table class="imgtable">
    <tr>
        <td>
            <a href="./"><img src="./cat.jpg" alt="" height="210px" /></a>&nbsp;
        </td>
        <td align="left">
            <p><a href="./"><font size="4">Huan Zheng </font>&nbsp<font size="4"; font style="font-family:Microsoft YaHei">郑欢</font><font size="4"></font></a><br />
            <br />

            <i>Incoming Ph.D candidate</i><br />
            Email: huanzheng1998@gmail.com<br />
            [<a href="https://scholar.google.com/citations?user=XnV4edcAAAAJ&hl=zh-CN&oi=sra" target="_blank">Google Scholar</a>]
            [<a href="https://github.com/Ian0926" target="_blank">GitHub</a>]<br />
            <br />

            Vision and Self-Driving Lab, Computer and Information Science, <a href="https://www.um.edu.mo/" target="_blank"> University of Macau (UM)</a><br />
            University of Macau, Avenida da Universidade, Taipa, Macau, China<br />
            <class="staffshortcut">
            <A HREF="#Bio">Bio</A> |
            <A HREF="#Publications">Publications</A> |
            <A HREF="#Preprints">Preprints</A> |
            <A HREF="#Challenge Reports">Challenge Reports</A> |
            <A HREF="#Awards">Awards</A> |
            <A HREF="#Services">Services</A>
            <br />
        </td>
    </tr>
</table>

<A NAME="Bio"><h2>Bio</h2></A>

I am an incoming Ph.D candidate at University of Macau (UM), supervised by Prof. <a href="https://shenjianbing.github.io/#-course">Jianbing Shen</a>.
Prior to this, I received my master's degree from Hefei University of Technology (HFUT), supervised by Prof. <a href="https://sites.google.com/site/cszzhang/" target="_blank">Zhao Zhang</a>.
I received my bachelor's degree from Northwest Agriculture and Forestry University (NWAFU).
I am interested in <b>image/video restoration and enhancement</b> and <b>BEV perception</b>.


<!-- <A NAME="News"><h2>News</h2></A>
<ul>
    <li> <b> <font color="#FF0000">[2023.05]</font> </b> Two papers are accepted to <b>IEEE TCE</b>.
    <li> <b> <font color="#FF0000">[2022.09]</font> </b> Recieve <b>National Scholarship</b>.
    <li> <b> <font color="#FF0000">[2022.07]</font> </b> Two papers are accepted to <b>ACM MM</b> 22. 
    <li> <b> <font color="#FF0000">[2022.03]</font> </b> One paper is accepted to <b>IEEE CVPR</b> 22.
    <li> <b> <font color="#FF0000">[2021.10]</font> </b> Recieve <b>National Scholarship</b>.
    <li> <b> <font color="#FF0000">[2021.07]</font> </b> One paper is accepted to <b>ACM MM</b> 21. 
</ul> -->

<A NAME="Publications"><h2>Publications</h2></A>
<font size="3"> 
<p style="text-indent: 0rem;margin-left: 0rem;"><sup>†Equal Contribution, * Corresponding Author(s)</sup></p>
<ul>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [6] Zhicheng Ji<sup>†</sup>, <b>Huan Zheng</b><sup>†</sup>, Zhao Zhang*, Qiaolin Ye, Yang Zhao and Mingliang Xu, 
        <a href="https://ian0926.github.io/" target="_blank">
            Multi-Scale Interaction Network for Low-Light Stereo Image Enhancement, 
        </a>
        <i>IEEE Transactions on Consumer Electronics (<b>IEEE TCE</b>)</i> 2023.
        &nbsp<a href="https://ian0926.github.io/" target="_blank">code</a>
    </span></p>
    
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [5] Zhao Zhang, <b>Huan Zheng</b>, Richang Hong*, Jicong Fan*, Yi Yang and Shuicheng Yan, 
        <a href="https://www.techrxiv.org/articles/preprint/FRC-Net_A_Simple_Yet_Effective_Architecture_for_Low-Light_Image_Enhancement/19771120/2" target="_blank">
            FRC-Net: A Simple Yet Effective Architecture for Low-Light Image Enhancement, 
        </a>
        <i>IEEE Transactions on Consumer Electronics (<b>IEEE TCE</b>)</i> 2023.
        &nbsp<a href="https://ian0926.github.io/" target="_blank">code</a>
    </span></p>
    
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [4] Yanyan Wei, Zhao Zhang*, <b>Huan Zheng</b>, Richang Hong, Yi Yang and Meng Wang, 
        <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548241" target="_blank">
            SGINet: Toward Sufficient Interaction Between Single Image Deraining and Semantic Segmentation,
        </a>
        <i>ACM International Conference on Multimedia (<b>ACM MM</b>)</i> 2022.
        &nbsp<a href="https://github.com/OaDsis/SGINet" target="_blank">code</a>
    </span></p>
    
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [3] <b>Huan Zheng</b>, Zhao Zhang*, Haijun Zhang, Yi Yang, Shuicheng Yan and Meng Wang, 
        <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548030" target="_blank">
            Deep Multi-Resolution Mutual Learning for Image Inpainting, 
        </a>
        <i>ACM International Conference on Multimedia (<b>ACM MM</b>)</i> 2022.
        &nbsp<a href="https://ian0926.github.io/" target="_blank">code</a>
    </span></p>
    
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [2] Zhao Zhang*, <b>Huan Zheng</b>, Richang Hong, Mingliang Xu, Shuicheng Yan and Meng Wang, 
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Deep_Color_Consistent_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf" target="_blank">
            Deep Color Consistent Network for Low Light-Image Enhancement, 
        </a>
        <i>IEEE Conference on Computer Vision and Pattern Recognition (<b>IEEE CVPR</b>)</i> 2022.
        &nbsp<a href="https://github.com/Ian0926/DCC-Net" target="_blank">code</a>
    </span></p>

    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [1] <b>Huan Zheng</b>, Zhao Zhang*, Yang Wang, Zheng Zhang, Mingliang Xu, Yi Yang and Meng Wang, 
        <a href="https://dl.acm.org/doi/10.1145/3474085.3475433" target="_blank">
            GCM-Net: Towards Effective Global Context Modeling for Image Inpainting, 
        </a>
        <i>ACM International Conference on Multimedia (<b>ACM MM</b>)</i> 2021.
        &nbsp<a href="https://github.com/Ian0926/GCM-Net" target="_blank">code</a>
    </span></p>
</ul>

<A NAME="Preprints"><h2>Preprints</h2></A>
<font size="3"> 
<ul>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [1] <b>Huan Zheng</b>, Zhao Zhang*, Jicong Fan, Richang Hong, Yi Yang, and Shuicheng Yan, 
        <a href="https://arxiv.org/abs/2211.00859" target="_blank">
            Decoupled Cross-Scale Cross-View Interaction for Stereo Image Enhancement in The Dark, 
        </a>
        <i><b>arxiv</b></i> 2022.
        &nbsp<a href="https://github.com/Ian0926/DCI-Net" target="_blank">code</a>
    </span></p>
</ul> 

<A NAME="Challenge Reports"><h2>Challenge Reports</h2></A>
<font size="3"> 
<ul>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [8] <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Ancuti_NTIRE_2023_HR_NonHomogeneous_Dehazing_Challenge_Report_CVPRW_2023_paper.pdf" target="_blank">
            NTIRE 2023 HR NonHomogeneous Dehazing Challenge Report, 
        </a>
        <b>CVPR NTIRE</b> 2023.
    </span></p>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [7] <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.pdf" target="_blank">
            NTIRE 2023 challenge on image denoising: Methods and results, 
        </a>
        <b>CVPR NTIRE</b> 2023.
    </span></p>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [6] <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhang_NTIRE_2023_Challenge_on_Image_Super-Resolution_x4_Methods_and_Results_CVPRW_2023_paper.pdf" target="_blank">
            NTIRE 2023 challenge on image super-resolution (x4): Methods and results, 
        </a>
        <b>CVPR NTIRE</b> 2023.
    </span></p>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [5] <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Vasluianu_NTIRE_2023_Image_Shadow_Removal_Challenge_Report_CVPRW_2023_paper.pdf" target="_blank">
            NTIRE 2023 image shadow removal challenge report, 
        </a>
        <b>CVPR NTIRE</b> 2023.
    </span></p>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [4] <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Wang_NTIRE_2023_Challenge_on_Stereo_Image_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.pdf" target="_blank">
            NTIRE 2023 challenge on stereo image super-resolution: Methods and results, 
        </a>
        <b>CVPR NTIRE</b> 2023.
    </span></p>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [3] <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_NTIRE_2023_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.pdf" target="_blank">
            NTIRE 2023 challenge on efficient super-resolution: Methods and results, 
        </a>
        <b>CVPR NTIRE</b> 2023.
    </span></p>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [2] <a href="https://arxiv.org/abs/2305.13770" target="_blank">
            MIPI 2023 Challenge on Nighttime Flare Removal: Methods and Results, 
        </a>
        <b>CVPR MIPI</b> 2023.
    </span></p>
    <p style="text-indent: -2rem;margin-left: 0rem;"><span>
        [1] <a href="https://arxiv.org/pdf/2209.07052.pdf" target="_blank">
            MIPI 2022 Challenge on Under-Display Camera Image Restoration: Methods and Results, 
        </a>
        <b>ECCV MIPI</b> 2022.
    </span></p>
</ul> 
 
<A NAME="Awards"><h2>Awards</h2></A>
<font size="3">
<ul>
    <li>2023 <b>Outstanding Graduate Student, Anhui Province</b></li>
    <li>2023 Outstanding Graduate Student, Hefei University of Technology</li>
    <li>2022 <b>Anhui Provincial Postgraduate Innovation Star</b></li>
    <li>2022 <b>National Scholarship</b></li>
    <li>2022 Rank 5 in ECCV MIPI-challenge Image Restoration for Under-display Camera Track</li>
    <li>2021 <b>National Scholarship</b></li>
</ul>

<A NAME="Services"><h2>Services</h2></A>
<font size="3">
<p style="text-indent: 0rem;margin-left: 0rem;"><b>Journal Reviewer</b></p>
<ul>
    <li>IEEE Transactions on Image Processing (<b>TIP</b>)</li>
    <li>Pattern Recognition (<b>PR</b>)</li>
    <li>Neural Networks (<b>NN</b>)</li>
</ul>
<p style="text-indent: 0rem;margin-left: 0rem;"><b>Conference Reviewer</b></p>
<ul>
    <li>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) (2023)</li>
    <li>IEEE International Conference on Computer Vision (<b>ICCV</b>) (2023)</li>
    <li>ACM International Conference on Multimedia (<b>ACM MM</b>) (2023)</li>
    <li>British Machine Vision Conference (<b>BMVC</b>) (2021)</li>
</ul>
    
<div id="footer">
    <div id="footer-text"></div>
</div>

<div>
    <center>
    <script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5cq5j1nt593&amp;s=350&amp;m=0&amp;v=true&amp;r=false&amp;b=ffffff&amp;n=true&amp;c=ff0000" async="async"></script>
    <br />© Huan Zheng
    </center>        
</div>
    
<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {
            $("#back_top").fadeIn(400);
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){

        $("html,body").animate({scrollTop:"0px"}, 200);

    });

});
</script>

</body>
</html>
